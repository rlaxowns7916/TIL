# S3 (Simple Storage Service)
- AWS에서 제공하는 **Object Storage Service**로, 데이터를 객체 단위로 저장한다.
- **Directory 개념이 없으며, Flat한 구조**로 되어 있다. 
  - Key에 `/`가 포함되면 콘솔에서는 디렉토리처럼 보이지만, 이는 단지 문자열 파싱 결과일 뿐이다.
- 다양한 용도로 사용된다:
    - 정적 웹 리소스 저장
    - 백업(Backup)
    - 재해 복구(DR)
    - 데이터 아카이빙
    - 분석을 위한 대용량 로그 저장 등
- 같은 prefix 당 초당 3500개의 PUT/COPY/POST/DELETE, 5500개의 GET을 지원한다.
  - prefix 기준으로 파티셔닝 되어 있기 때문에 기준이된다.

---

## Object
- Bucket에 저장되는 파일 단위의 데이터
- 각각의 Object는 고유한 **Key**를 가지며, 이 Key는 **전체 경로(Full Path)**로 간주된다. ex) `/2025/04/04/test.txt`
- 하나의 Object 크기:
    - 최대 **5TB**
    - 단일 PUT 요청은 **최대 5GB**까지만 지원되며, 이를 초과하면 **Multipart Upload**를 사용해야 한다. (권장은 100MB 이상)

### Metadata & Tags
- Object에 대한 추가적인 정보를 제공하기 위해서 사용한다.
  - **metadata나 tag로 검색을 할 수는 없다.**
- Metadata: 객체에 대한 추가 정보를 담고 있는 Key-Value 쌍
    - 시스템 메타데이터: S3에서 자동으로 생성되는 메타데이터 (ex. 크기, 생성일 등)
    - 사용자 정의 메타데이터(prefix: x-amz-meta-): 사용자가 정의한 메타데이터 (ex. 버전, 작성자 등)
    - 객체 검색 시 얻어올 수 있다.
- Tag
  - 객체에 대한 추가 정보를 담고 있는 Key-Value 쌍
  - 객체에 대한 태그를 사용하여 객체를 분류하거나 관리할 수 있다. (분석 목적에 유용)

---
## Storage Class 
- object 단위로 설정된다.
    - 즉 하나의 Bucket에도 다양한 Storage Class를 가지는 Object들이 존재할 수 있다.
- **LifeCycle 설정을 통해서 Class 간 이동을 정의할 수 있다.**
  - ex) standard --(30일간 접근이 없다면)---> IA
### [1] Standard
- general purpose storage
- Durability(내구성-99.999999999%, 11 9's)와 가용성(Availability-99.99%)을 제공한다.
    - Durability만약 10,000,000개의 Object를 저장한다면 10,000년당 한개의 Object가 손실될 확률이 있음
    - Availability: 1년 중 53분의 Downtime이 발생할 수 있음

### [2] IA (Standard-Infrequence-Access)
- 자주 사용하지 않지만, 빠른 접근이 필요한 데이터
- standard보다 비용이적지만, 접근할 때마다 비용이 발생한다.
- 주로 Backup이나 DR목적으로 사용한다.
- Durability(내구성-99.999999999%, 11 9's)와 가용성(Availability-99.9%)을 제공한다.

### [3] One Zone-IA (One Zone-Infrequence-Access)
- IA와 비슷하지만, 단일 AZ에 저장된다.
    - AZ가 장애가 나면 데이터 손실이 발생할 수 있다.
- 주로 재생성 가능한 데이터의 Secondary Backup 용도로 사용된다.
- Durability(내구성-99.999999999%, 11 9's)와 가용성(Availability-99.5%)을 제공한다.

### [4] Glacier Instance Retrieval
- 저렴한 비용으로 장기 보관이 필요한 데이터
    - 최소 90일간은 저장해야 한다.
- BackUp 데이터이지만, 저 지연으로 데이터에 접근해야할 때 사용한다.
    - 검색에 milliSecond 단위의 지연이 발생한다.
    - 분기에 한번정도 데이터에 접근할 때 유용하다.

### [5] Glacier Flexible Retrieval
- 저렴한 비용으로 장기 보관이 필요한 데이터
    - 최소 90일간은 저장해야 한다.
- 3개의 검색 옵션을 제공한다. (요금차이 존재)
    - Expedited: 1~5분
    - Standard: 3~5시간
    - Bulk: 5~12시간(free)

### [6] Glacier Deep Archive
- 가장 저렴한 저장 클래스이며, 장기보관 하지만 오랫동안 접근하지 않는 데이터
    - 최소 180일간은 저장해야 한다.
    - Archiving, Compliance 준수 등 목적으로 사용한다.
- 2개의 검색옵션을 제공한다. (요금차이 존재)
    - Standard: 12시간
    - Bulk: 48시간

### [7] Intelligent-Tiering
- 데이터 접근 패턴이 불규칙하거나 예측하기 어려운 경우에 적합한 자동 계층화 저장 클래스
- 월별 Object Monitoring과 Class 이동비용이 발생한다.
    - 검색에는 비용이 발생하지 않는다.
- 아래와 같은 계층으로 나뉜다.
    - Frequent Access Tier(automatic): 자주 접근하는 데이터 (default)
    - Infrequent Access Tier(automatic): 30일간 접근하지 않은 데이터
    - Archive Instant Access Tier(automatic): 90일간 접근하지 않은 데이터
    - Archive Access Tier(automatic): 90~700+일(설정가능)간 접근하지 않은 데이터
    - Deep Archive Access Tier(automatic): 180~700+일(설정가능)간 접근하지 않은 데이터
---
## Bucket
- 객체(Object)를 저장하기 위해 **반드시 Bucket을 생성**해야 한다.
- Bucket은 S3에서 **최상위 네임스페이스**로 동작하며, 전 세계적으로 **유일한 이름**이어야 한다.
- Bucket은 **Region 단위로 생성**되며, 생성 후 Region은 변경할 수 없다.
- Bucket 이름 규칙:
    - 길이는 **3~63자**
    - **소문자, 숫자, 하이픈(-)** 만 사용 가능
    - **대문자 및 밑줄(_)은 허용되지 않음**
    - IP 주소 형식(예: `192.168.1.1`)도 사용할 수 없음
    - DNS 호환성을 고려하여 명명해야 함


# Multipart Upload
- S3에서 제공하는 대용량 파일 업로드 방식
  - 5GB 이상의 파일을 업로드할 때 강제
- Chunk 단위(최대 10000)로 나누어 병렬로 업로드하며, 각 Chunk는 독립적으로 업로드 가능
  - 모든 Chunk가 업로드되면, S3가 내부적으로 병합하여 하나의 Object로 만들어낸다.
  - 실패한 Part만 재업로드가 가능하기 때문에 효율적이다.
  - 병렬로 업로드하기 때문에 속도가 빠르다.

## 주의해야 할 점
- 순서가 중요하다.
  - Complete 요청 시, 순서대로 정렬되어 있어야한다.
  - 응답으로 받은 ETag를 순서대로 정렬하여 요청해야한다.
- UploadId는 추적해야한다.
  - 실패한 Part를 재업로드할 때 UploadId가 필요하다.
  - initiate-upload를 요청하면, 서버에서 UploadId를 발급해준다.
- 업로드 실패하거나, Abort되었을 떄의 자원을 해제해주어야한다. (AbortMultipartUpload)
- 실행순서
  1. Initiate Multipart Upload (UploadId 발급)
  2. Upload Part (Chunk 단위로 업로드 / 응답으로 ETag 발급)
  3. Complete Multipart Upload (모든 Part를 병합하여 Object 생성)
     - 응답으로 받은 ETag를 순서대로 정렬하여 요청해야한다.
     - ```xml
        <CompleteMultipartUpload>
           <Part>
             <PartNumber>1</PartNumber>
             <ETag>"etag-part-1"</ETag>
           </Part>
           <Part>
             <PartNumber>2</PartNumber>
             <ETag>"etag-part-2"</ETag>
           </Part>
           <Part>
             <PartNumber>3</PartNumber>
             <ETag>"etag-part-3"</ETag>
           </Part>
        </CompleteMultipartUpload>
       ```
---

## Pre-Signed URL
- 인증이 필요한 객체에 대해 **일시적인 접근 권한을 부여**하기 위한 URL
  - S3-Console: 1min ~ 12hour
  - AWS CLI: 1min ~ 168hour (default: 1hour)
- 특정 IAM 권한을 가진 사용자가 생성 가능하며, **유효기간(expiration)** 설정이 필수
  - 생성한 사용자의 권한을 이어받는 URL이 생기는 것이다.
- GET, PUT 등 다양한 S3 작업에 사용 가능
- 예시 사용 시나리오:
    - 인증되지 않은 외부 사용자에게 일시적 다운로드 권한 제공
    - 클라이언트 업로드 시 S3에 직접 업로드 유도

---

## S3 보안 (S3 Security)

### [1] User-Based 접근 제어

- **IAM 정책(IAM Policy)** 사용
    - 특정 사용자, 그룹, 역할에 대해 S3 작업 권한을 부여
    - 예: `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`
    - 정책은 사용자 또는 역할에 연결되어 직접 접근 권한을 부여함

### [2] Resource-Based 접근 제어

- **Bucket Policy**
    - 버킷 단위로 설정되는 JSON 형식의 정책
    - 특정 리소스에 대한 접근을 제한 또는 허용
    - 예시:
      ```json
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::example-bucket/*"
          }
        ]
      }
      ```
    - 주요 필드:
        - `Effect`: `Allow` 또는 `Deny`
        - `Principal`: 접근 주체 (IAM 사용자, 역할, 계정 등)
        - `Action`: 허용/제한할 작업 (예: `s3:GetObject`)
        - `Resource`: 정책이 적용될 대상 (버킷 또는 객체)

- **ACL (Access Control List)**
    - 객체 또는 버킷 단위의 레거시 권한 부여 방식
    - 간단한 접근 제어만 가능하며, 최신 환경에서는 권장되지 않음

---

## Encryption (암호화)

- S3는 객체 단위의 암호화를 지원
- 암호화 방식:
    - **SSE-S3**: AWS 관리형 키로 서버 측 암호화
    - **SSE-KMS**: 사용자가 직접 제어하는 KMS 키를 사용
    - **SSE-C**: 클라이언트 제공 키로 암호화 (비추천)
- 또한, 클라이언트 측에서 직접 암호화한 뒤 업로드하는 방식도 사용 가능

---

## Object Versioning (객체 버전 관리)
- 동일한 Key로 객체를 업로드할 때 **새로운 버전**이 생성된다.
- 활성화는 Bucket 단위로 설정되며, **기본적으로 비활성화** 되어 있다.
    - 실수로 덮어쓴 파일 복구 가능
    - 삭제된 객체 복구 가능
    - Object Version 설정이 되기 이전 객체의 Version은 null이 된다.
- 중간버전을 반환하고 싶으면, 중간버전을 최신버전으로 만드는 방법 밖에 없다. (최신 버전들을 삭제)
- Delete Marker
  - key에 대해 **Delete Marker**가 생성되며, 이 Marker는 객체가 삭제된 것처럼 보이게 한다.
  - **Object Versioning**이 활성화된 Bucket에서 객체(Object)를 삭제하면, 실제 데이터를 삭제하는 것이 아니라 **Delete Marker**라는 특별한 버전이 생성된다.
      - Delete Marker는 **"삭제되었다는 표시만 존재"**이며, 해당 Key에 대해 S3가 더 이상 기본적으로 객체를 반환하지 않도록 만든다.
  - 특정 버전에 대해서 Marking하는 것이 아닌, Key전체에 대한 것이다.
  - Delete Marker가 생성된 객체는 접근하면 **404 Not Found** 에러가 발생한다.
  - Delete Marker를 삭제하면, 해당 Key에 대한 가장 최근의 유효한(=Delete Marker가 아닌) 버전이 자동으로 복원된다.

---

## Replication
- Replication 옵션이 Enable 된 이후의 객체부터 복제된다.
  - 기존 객체를 복제하려면 S3 Batch Replication을 사용해야한다. (기존 객체 및 Replication 실패한 객체를 복제할 때 사용하는 기능)
- ObjectVersioning이 Enable 되어 있어야 한다.
  - DeleteMarker도 복제 가능하다. (Optional)
- Replication Chaining은 지원하지 않는다.
  - ex) A(source)에서 B,C(destination)로 복제하려고 할 때, A->B, B->C 구성은 지원하지 않는다. A->B, A->C가 지원된다.

- SRR (Same Region Replication)
    - 동일한 리전 내에서 객체를 복제
      - log aggregation
      - 데이터 분석
    - 버킷 단위로 설정 가능
    - 버전 관리가 활성화된 버킷에서만 사용 가능
- CRR (Cross Region Replication)
    - 서로 다른 리전 간에 객체를 복제
      - 컴플라이언스 준수
      - 재해 복구(DR) 목적
      - 지연시간 감소
    - 버킷 단위로 설정 가능
    - 버전 관리가 활성화된 버킷에서만 사용 가능

---

## S3 Event Notifications
- S3에서 발생하는 이벤트에 대해 알림을 받을 수 있는 기능
  - S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication, ...
- 이벤트 발생 시, SNS, SQS, Lambda 등으로 알림을 전송할 수 있다.
  - Resource Policy를 사용하여 S3가 SNS, SQS, Lambda에 접근할 수 있도록 설정해야 한다.
- EventBridge를 사용하여 S3 이벤트를 수집하고, 이를 기반으로 다양한 AWS 서비스와 통합할 수 있다.
  - 기존 3개 서비스(SNS, SQS, Lambda) 외에도 다양한 서비스와 통합 가능
  - event에 대한 filtering rule도 설정 가능하다. (metadata, object size, name, ...)

## Transfer Acceleration
- S3 Transfer Acceleration은 S3에 업로드하는 속도를 높이기 위한 기능
- CloudFront의 Edge Location을 사용하여 전송 속도를 향상시킨다.


## ByteRange Fetch
- S3는 객체를 바이트 단위로 요청할 수 있는 기능을 제공한다.
- 특정 바이트 범위의 데이터를 요청할 수 있으며, 이를 통해 대용량 객체를 효율적으로 처리할 수 있다. (병렬처리)

## S3 Select
- S3 Select는 S3에 저장된 객체에서 필요한 데이터만 선택적으로 조회할 수 있는 기능
- CSV, JSON, Parquet 형식의 객체에서 SQL 쿼리를 사용하여 데이터를 필터링하고 반환한다.
```sql
SELECT s.name, s.age
FROM S3Object s
WHERE s.age > 30
```

## Object Encryption

### [1] Server-Side Encryption (SSE)
1. SSE-S3
   - AWS에서 관리하는 키로 암호화
   - 기본적으로 사용되며, 별도의 설정 없이 사용 가능
   - **x-amz-server-side-encryption: AES256** 헤더를 설정해야 한다.
2. SSE-KMS
    - AWS KMS에서 관리하는 키로 암호화
      - cloud-trail을 통해서 kms 접근에 대한 추적이 가능하다는 것도 장점
      - **kms는 초당 호출 횟수 제한이 있을 수 있기 때문에, 주의해야 한다.**
    - 사용자가 직접 키를 관리할 수 있으며, IAM 정책을 통해 세부적인 권한 제어 가능
      - aws가 제공하는 key는 무료이며, 직접 생성한 key는 비용이 발생한다.
    - **x-amz-server-side-encryption: aws:kms** 헤더를 설정해야 한다.
3. SSE-C
   - 콘솔에서만 설정 가능
   - 클라이언트가 제공하는 키로 암호화
     - HTTPS가 필수적이며, 헤더 정보에 포함해서 보낸다.
   - AWS는 키를 저장하지 않으며, 사용자가 직접 관리해야 한다.
   - **x-amz-server-side-encryption-customer-algorithm: AES256** 헤더를 설정해야 한다.
   - **x-amz-server-side-encryption-customer-key** 헤더를 설정해야 한다.
   - **x-amz-server-side-encryption-customer-key-MD5** 헤더를 설정해야 한다.

### [2] Client-Side Encryption
- 클라이언트에서 직접 암호화하여 S3에 업로드
- S3는 암호화된 데이터를 저장하며, 복호화는 클라이언트에서 수행

### [3] Encryption in Transit
- Encryption in Transit은 SSL/TLS라고도 불린다.
  - HTTP: 암호화(X)
  - HTTPS: 암호화(O)
- BucketPolicy > aws:SecureTransport: True를 통해서 Https를 강제 할 수 있다.
  - BucketPolicy는 Default Encryption보다 먼저 평가된다.

## S3 CORS
- Origin: Protocol + Domain + Port
  - **CORS는 기본적으로 웹브라우저에서 js를 통해서 다른 도메인에 접근할 때 발생하는 문제를 해결하기 위한 것이다. (서버 단 X)**
  - Client에서 직접 S3에 접근할 때 발생하는 문제를 해결하기 위한 것이다.
- 기본적으로 S3는 CORS를 허용하지않는다. (CORS 설정 필요)
  - 특정 도메인이나 전체에게 열어줄 수 있다.
  - ```xml
    <CORSConfiguration>
        <CORSRule>
            <AllowedOrigin>*</AllowedOrigin>
            <AllowedMethod>GET</AllowedMethod>
            <AllowedMethod>HEAD</AllowedMethod>
        </CORSRule>
     </CORSConfiguration>
    ```
## Access Log
- S3에서 발생하는 모든 요청에 대한 로그를 기록하는 기능
- Audit 목적이 강하며, Authorized / Denied 요청을 모두 기록한다.
- 파일로 다른 Bucket에 저장된다.
  - 대상 Bucket은 같은 AWS Region 내에 위치해야한다.
  - **로깅할 Bucket과 원본 Bucket을 분리해야한다.** (무한루프가 발생 하기 때문이다.)
